FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04 
# Reference: https://github.com/mozilla/DeepSpeech
# Pitfall: We gave to use the image of 9.0 version, instead of 10.0 version, though the official document says that it is supported with cuda 10.0.
# If you use nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04, "ImportError: libcusolver.so.9.0: cannot open shared object file: No such file or directory"
# would appear. This is essentially just a naming problem, but annoying.
# In addition, when running this container, have to put --runtime=nvidia as parameter. As it also uses GPU.
RUN apt-get update
RUN apt-get install wget unzip python3 python3-pip -y
RUN pip3 install grpcio
RUN pip3 install protobuf
RUN apt-get install curl -y
RUN apt-get install ffmpeg -y
RUN pip3 install deepspeech-gpu

ADD imagequery/c1_speechRecognition/app /container/
ADD imagequery/c1_speechRecognition/data /container/data

# dataset1 
# dataset source: http://festvox.org/cmu_arctic/
RUN wget --progress=bar:force http://festvox.org/cmu_arctic/cmu_arctic/packed/cmu_us_awb_arctic-0.95-release.zip
RUN mv $(find / -name cmu_us_awb_arctic-0.95-release.zip) /container/data/dataset1
RUN chmod 777 /container/data/dataset1/unzipRename.sh
RUN /container/data/dataset1/unzipRename.sh

# dataset2
# reference: https://groups.csail.mit.edu/sls/downloads/flickraudio/index.cgi
RUN wget --progress=bar:force https://groups.csail.mit.edu/sls/downloads/flickraudio/downloads/flickr_audio.tar.gz
RUN mv $(find / -name flickr_audio.tar.gz) /container/data/dataset2
RUN chmod 777 /container/data/dataset2/untarRename.sh 
RUN /container/data/dataset2/untarRename.sh

# dataset3
# download reference: https://www.kaggle.com/general/6604
# dataset refernce: https://www.kaggle.com/rtatman/speech-accent-archive/
RUN curl 'https://storage.googleapis.com/kaggle-datasets/4114/6391/speech-accent-archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1559281077&Signature=DEELrKnXgU1zhlccGQF7RaYhQK66DwnGc52mnzOXpjNkzWXS5FxjvwsyRM3JoeaaZ%2F4oDszZKq%2BJB%2Brd7hHrP%2FGC8EDdJLxMdUkBOZyt2bHLUTdrs%2BdYLZDfFHZJlXAq%2Fp3E0%2FsnMrgQVru9wCSt72oCItrKUJwfW3H%2FSeVmNv6qkuPanvAT3AldueMGKLZLxxtzBjebuyyBMM8hqqVOui2lXw9Tokt9jQR45x8eqieRb%2FqFczap2r51EwOYqm%2BLIzFlDzL0CLpTfy5JnyqTvtNzCTQqHXj6UWae5E9ewGGczKBm%2F%2Bp95%2FRNXDu5tH9utllMqSnz%2Bl7PJ5hG2oxA4g%3D%3D' -H 'authority: storage.googleapis.com' -H 'pragma: no-cache' -H 'cache-control: no-cache' -H 'upgrade-insecure-requests: 1' -H 'user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36' -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3' -H 'referer: https://www.kaggle.com/' -H 'accept-encoding: gzip, deflate, br' -H 'accept-language: en,zh;q=0.9,zh-CN;q=0.8' --compressed -o speech-accent-archive.zip
RUN mv $(find / -name speech-accent-archive.zip) /container/data/dataset3 
RUN ls /container/data/dataset3
RUN unzip /container/data/dataset3/speech-accent-archive.zip -d /container/data/dataset3 
# /container/data/dataset3 now contains:reading-passage.txt, recordings.zip, speakers_all.csv
RUN unzip /container/data/dataset3/recordings.zip -d /container/data/dataset3/
RUN chmod 777 /container/data/dataset3/rename.sh
RUN /container/data/dataset3/rename.sh

# download pre-trained model
RUN wget --progress=bar:force https://github.com/mozilla/DeepSpeech/releases/download/v0.4.1/deepspeech-0.4.1-models.tar.gz
RUN tar xvfz $(find / -name deepspeech-0.4.1-models.tar.gz)
RUN mv /models /container/models

# /container has: predict.py, data, models
# /container/models has: alphabet.txt, lm.binary, output_graph.pb, output_graph.pbmm. output_graph.rounded.pb, output_graph.rounded.pbmm, trie

ADD grpc/app/container_entry.sh /container/
ADD grpc/app/server.py /container/
ADD grpc/app/model_pb2_grpc.py /container/
ADD grpc/app/model_pb2.py /container/
ADD grpc/app/proxy_pb2_grpc.py /container/
ADD grpc/app/proxy_pb2.py /container/

# for predict.py
RUN apt-get install sox -y 
# for pydub
RUN pip3 install pydub -y 
RUN apt-get install libav-tools libavcodec-extra 
# for truncate.py
RUN pip3 install glob -y 
RUN python3 /container/data/dataset3/truncate.py
CMD ["python3", "/container/predict.py", "--model", "/container/models/output_graph.pbmm", "--alphabet", "/container/models/alphabet.txt", "--audio", "/container/data/dataset3/recordings/1.wav"]
# CMD ["/container/container_entry.sh", "c1", "/container/server.py"]

EXPOSE 8000