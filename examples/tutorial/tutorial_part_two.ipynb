{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipper Tutorial: Part 2\n",
    "\n",
    "In this part of the tutorial, you will put on your data scientist hat and train and deploy some models to Clipper to improve your application accuracy.\n",
    "\n",
    "\n",
    "# Connect to Clipper (again)\n",
    "\n",
    "Because this is a separate Python instance, you must create a new `ClipperConnection` object and connect to your running Clipper cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from clipper_admin import ClipperConnection, DockerContainerManager\n",
    "clipper_conn = ClipperConnection(DockerContainerManager())\n",
    "clipper_conn.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cifar\n",
    "\n",
    "Because this is a new notebook, you must load the CIFAR dataset again. This time, you will be using it to train and evaluate machine learning models.\n",
    "\n",
    "Set `cifar_loc` to the same location you did in the \"Download the Images\" section of part one of the tutorial. You will load into Python the number of training and test datapoints specified in \"Extract the images\" section of part one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar_loc = \"\"\n",
    "import cifar_utils\n",
    "train_x, train_y = cifar_utils.filter_data(\n",
    "    *cifar_utils.load_cifar(cifar_loc, cifar_filename=\"cifar_train.data\", norm=True))\n",
    "test_x, test_y = cifar_utils.filter_data(\n",
    "    *cifar_utils.load_cifar(cifar_loc, cifar_filename=\"cifar_test.data\", norm=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Logistic Regression Model\n",
    "\n",
    "When tackling a new problem with machine learning, it's always good to start with simple models and only add complexity when needed. Start by training a logistic regression binary classifier using [Scikit-Learn](http://scikit-learn.org/). This model gets about 68% accuracy on the offline evaluation dataset if you use 10,000 training examples. It gets about 74% if you use all 50,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm \n",
    "def train_sklearn_model(m, train_x, train_y):\n",
    "    m.fit(train_x, train_y)\n",
    "    return m\n",
    "lr_model = train_sklearn_model(lm.LogisticRegression(), train_x, train_y)\n",
    "print(\"Logistic Regression test score: %f\" % lr_model.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Logistic Regression Model\n",
    "\n",
    "While 68-74% accuracy on a CIFAR binary classification task is significantly below state of the art, it's already much better than the 50% accuracy your application yields right now by guessing randomly.\n",
    "\n",
    "To deploy your Scikit-Learn logistic regression model, you can use one of the provided Clipper model deployer modules. In particular, because the Scikit-Learn model can be pickled, you can wrap it in a function closure and\n",
    "deploy that function directly as a model to Clipper without needing to manually save the model or write a Docker\n",
    "container. To do this, you will import the `clipper_admin.deployers.python` module.\n",
    "\n",
    "First, let's write and test the prediction function we will deploy. This function must take a list of inputs (a list of CIFAR images in this example) as the only function argument and return a list of predictions as strings.\n",
    "\n",
    "> _The reason the prediction function takes a list of inputs rather than a single input is to allow deployed models the possibility of computing multiple predictions in parallel to improve model performance. For example, many models that run on a GPU can significantly improve throughput by batching predictions to better utilize the many parallel cores of the GPU._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sklearn_predict(xs):\n",
    "    preds = lr_model.predict(xs)\n",
    "    return [str(p) for p in preds]\n",
    "\n",
    "# Test the function on the first four images in the CIFAR dataset\n",
    "sklearn_predict(test_x[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have defined a prediction function, you can deploy it directly to Clipper. When deploying a model, you must assign some additional metadata to the model. You must give it a unique name (\"sklearn-cifar\"), a version (1), and the input type it accepts (this input type must match any applications you link the model to). You can also optionally assign descriptive labels to the model and specify how many replicas of the model to launch. Each model replica runs in its own Docker container and so adding more replicas can increase the aggregate throughput of the model. You can always change the number of replicas of a model later by calling `clipper_conn.set_num_replicas()`.\n",
    "\n",
    "After completing this step, Clipper will be managing a new container in Docker with your model in it:\n",
    "<img src=\"img/deploy_sklearn_model.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "> *Once again, because you are deploying a Docker image this command may take awhile to download the image. Thanks for being patient!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clipper_admin.deployers import python as python_deployer\n",
    "\n",
    "model_name = \"birds-vs-planes-classifier\"\n",
    "\n",
    "python_deployer.deploy_python_closure(clipper_conn,\n",
    "                                      name=model_name,\n",
    "                                      version=1,\n",
    "                                      input_type=\"doubles\",\n",
    "                                      func=sklearn_predict\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link your app to your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your newly deployed model to generate predictions, it needs to be linked to your Clipper application. This tells Clipper to route requests for the `\"cifar-demo\"` app to the `\"birds-vs-planes-classifier\"` model to make predictions.\n",
    "\n",
    "When you link a model to an application, it automatically applies to all versions of that model. So if you update the model to a new version or rollback to an old one, all application links will be automatically preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_name = \"cifar-demo\"\n",
    "clipper_conn.link_model_to_app(app_name, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view which models your app is linked to by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.get_linked_models(app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've deployed and linked your model to your app, go ahead and check back on your running frontend application from part 1. You should see the accuracy rise from around 50% to the accuracy of your SKLearn model (68-74%), without having to stop or modify your application at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TensorFlow Model\n",
    "\n",
    "To improve the accuracy of your application further, you will now deploy a TensorFlow convolutional neural network. This model takes a few hours to train, so you will download the trained model parameters rather than training it from scratch. This model gets about 88% accuracy on the test dataset.\n",
    "\n",
    "There is a pre-trained TensorFlow model stored in the repo using [`git-lfs`](https://git-lfs.github.com/). Once you install `git-lfs`, you can download the model with the command `git lfs pull`. If you don't want to deploy a TensorFlow model, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf_cifar_model_path = os.path.abspath(\"tf_cifar_model/cifar10_model_full\")\n",
    "tf_session = tf.Session('', tf.Graph())\n",
    "with tf_session.graph.as_default():\n",
    "    saver = tf.train.import_meta_graph(\"%s.meta\" % tf_cifar_model_path)\n",
    "    saver.restore(tf_session, tf_cifar_model_path)\n",
    "\n",
    "def tensorflow_score(session, test_x, test_y):\n",
    "    \"\"\"\n",
    "    NOTE: This predict method expects pre-whitened (normalized) images\n",
    "    \"\"\"\n",
    "    logits = session.run('softmax_logits:0',\n",
    "                           feed_dict={'x:0': test_x})\n",
    "    relevant_activations = logits[:, [cifar_utils.negative_class, cifar_utils.positive_class]]\n",
    "    preds = np.argmax(relevant_activations, axis=1)\n",
    "    return float(np.sum(preds == test_y)) / float(len(test_y))\n",
    "print(\"TensorFlow CNN test score: %f\" % tensorflow_score(tf_session, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy TensorFlow Model\n",
    "\n",
    "Unlike the Scikit-Learn model, TensorFlow models cannot be pickled. Instead, they must be saved using TensorFlow's native serialization API. Because of this, you must save the model yourself and specify a Docker container that knows how to load and run a TensorFlow model. We have provided a Docker image for you, the `clipper/tf_cifar_container:latest`, that can run the CIFAR TensorFlow model. The Docker container will load and reconstruct the model from the serialized model checkpoint when the container is started.\n",
    "\n",
    "After completing this step and deploying the new model, Clipper will send queries to the newly-deployed TensorFlow model instead of the logistic regression Scikit-Learn model, improving the application's accuracy.\n",
    "<img src=\"img/tf_replaces_sklearn_model.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "> *Once again, please patient while the Docker image is downloaded.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=2,\n",
    "    input_type=\"doubles\",\n",
    "    model_data_path=os.path.abspath(\"tf_cifar_model\"),\n",
    "    base_image=\"clipper/tf_cifar_container:latest\",\n",
    "    num_replicas=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the accuracy of your frontend application a final time, you should see accuracy around 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Clipper Metrics\n",
    "\n",
    "Clipper also records various system performance metrics. You can inspect the current state of these metrics with the `inspect_instance()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.inspect_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Congratulations! You've now successfully completed the tutorial. You started Clipper, created an application and queried it from a frontend client, and deployed two models trained in two different machine learning frameworks (Scikit-Learn and TensorFlow) to the running system.__\n",
    "\n",
    "*Head back to the notebook from part 1. When you're done watching the accuracy of your application, stop the cell (hit the little \"stop\" square in the notebook toolbar).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/warning.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "> This step will stop and remove all Clipper Docker containers. This command will not affect any other running Docker containers.\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "\n",
    "__When you're completely done with the tutorial and want to shut down your Clipper instance, you can run the `stop_all()` command to stop all the Clipper Docker containers.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.stop_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
