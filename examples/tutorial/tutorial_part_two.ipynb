{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipper Tutorial: Part 2\n",
    "\n",
    "In this part of the tutorial, you will put on your data scientist hat and train and deploy some models to Clipper to improve your application accuracy.\n",
    "\n",
    "\n",
    "# Connect to Clipper (again)\n",
    "\n",
    "Because this is a separate Python instance, you must create a new `Clipper` object and connect to your running Clipper instance. Make sure you enter the same information here as you did in part one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipper_manager must be on your path:\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../management/'))\n",
    "import clipper_manager as cm\n",
    "# Change the username if necessary\n",
    "user = \"\"\n",
    "# Set the path to the SSH key\n",
    "key = \"\"\n",
    "# Set the SSH host\n",
    "host = \"\"\n",
    "clipper = cm.Clipper(host, user, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cifar\n",
    "\n",
    "Because this is a new notebook, you must load the CIFAR dataset again. This time, you will be using it to train and evaluate machine learning models.\n",
    "\n",
    "Set `cifar_loc` to the same location you did in the \"Download the Images\" section of part one of the tutorial. You will load into Python the number of training and test datapoints specified in \"Extract the images\" section of part one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_loc = \"\"\n",
    "import cifar_utils\n",
    "train_x, train_y = cifar_utils.filter_data(\n",
    "    *cifar_utils.load_cifar(cifar_loc, cifar_filename=\"cifar_train.data\", norm=True))\n",
    "test_x, test_y = cifar_utils.filter_data(\n",
    "    *cifar_utils.load_cifar(cifar_loc, cifar_filename=\"cifar_test.data\", norm=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Logistic Regression Model\n",
    "\n",
    "When tackling a new problem with machine learning, it's always good to start with simple models and only add complexity when needed. Start by training a logistic regression binary classifier using [Scikit-Learn](http://scikit-learn.org/). This model gets about 68% accuracy on the offline evaluation dataset if you use 10,000 training examples. It gets about 74% if you use all 50,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm \n",
    "def train_sklearn_model(m, train_x, train_y):\n",
    "    m.fit(train_x, train_y)\n",
    "    return m\n",
    "lr_model = train_sklearn_model(lm.LogisticRegression(), train_x, train_y)\n",
    "print(\"Logistic Regression test score: %f\" % lr_model.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Logistic Regression Model\n",
    "\n",
    "While 68-74% accuracy on a CIFAR binary classification task is significantly below state of the art, it's already much better than the 50% accuracy your application yields right now by guessing randomly.\n",
    "\n",
    "You can deploy your logistic regression model directly to Clipper without having to worry about how to serialize the model or integrate it with application code.\n",
    "\n",
    "To deploy a model to Clipper, you must assign it a name (\"sklearn_cifar\"), a version (1), and then provide some metadata about the model itself. In this case, you are specifying that you want to run the model using the `sklearn_cifar_container` Docker image in the [Clipper repo](https://hub.docker.com/u/clipper/dashboard/) on Docker Hub. You can assign the model descriptive labels, and specify the input type that this model expects. Finally, you can specify how many *replicas* of the model (how many Docker containers) to launch. Adding more replicas increases the throughput of this model.\n",
    "\n",
    "After completing this step, Clipper will be managing a new container in Docker with your model in it:\n",
    "<img src=\"img/deploy_sklearn_model.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "> *Once again, because you are deploying a Docker image this command may take awhile to download the image. Thanks for being patient!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"birds_vs_planes_classifier\"\n",
    "\n",
    "model_added = clipper.deploy_model(\n",
    "    model_name,\n",
    "    1,\n",
    "    lr_model,\n",
    "    \"clipper/sklearn_cifar_container:latest\",\n",
    "    [\"cifar\", \"sklearn\"],\n",
    "    \"doubles\",\n",
    "    num_containers=1\n",
    ")\n",
    "print(\"Model deploy successful? {success}\".format(success=model_added))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've deployed your model, go ahead and check back on your running frontend application from part 1. You should see the accuracy rise from around 50% to the accuracy of your SKLearn model (68-74%), without having to stop or modify your application at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TensorFlow Model\n",
    "\n",
    "To improve the accuracy of your application further, you will now deploy a TensorFlow convolutional neural network. This model takes a few hours to train, so you will download the trained model parameters rather than training it from scratch. This model gets about 88% accuracy on the test dataset.\n",
    "\n",
    "There is a pre-trained TensorFlow model stored in the repo using [`git-lfs`](https://git-lfs.github.com/). Once you install `git-lfs`, you can download the model with the command `git lfs pull`. If you don't want to deploy a TensorFlow model, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf_cifar_model_path = os.path.abspath(\"tf_cifar_model/cifar10_model_full\")\n",
    "tf_session = tf.Session('', tf.Graph())\n",
    "with tf_session.graph.as_default():\n",
    "    saver = tf.train.import_meta_graph(\"%s.meta\" % tf_cifar_model_path)\n",
    "    saver.restore(tf_session, tf_cifar_model_path)\n",
    "\n",
    "def tensorflow_score(session, test_x, test_y):\n",
    "    \"\"\"\n",
    "    NOTE: This predict method expects pre-whitened (normalized) images\n",
    "    \"\"\"\n",
    "    logits = session.run('softmax_logits:0',\n",
    "                           feed_dict={'x:0': test_x})\n",
    "    relevant_activations = logits[:, [cifar_utils.negative_class, cifar_utils.positive_class]]\n",
    "    preds = np.argmax(relevant_activations, axis=1)\n",
    "    return float(np.sum(preds == test_y)) / float(len(test_y))\n",
    "print(\"TensorFlow CNN test score: %f\" % tensorflow_score(tf_session, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy TensorFlow Model\n",
    "\n",
    "Similar to deploying the logistic regression model, you can now deploy your TensorFlow neural network. Note that you will specify a different model container to use this time: the `tf_cifar_container`. In this case, you are providing Clipper a serialized version of the model. The container has been set up to reconstruct the original model from the serialized representation.\n",
    "\n",
    "After completing this step, Clipper will send queries to the newly-deployed TensorFlow model instead of the logistic regression Scikit-Learn model, improving the application's accuracy.\n",
    "<img src=\"img/tf_replaces_sklearn_model.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "> *Once again, please patient while the Docker image is downloaded.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_added = clipper.deploy_model(\n",
    "    model_name,\n",
    "    2,\n",
    "    os.path.abspath(\"tf_cifar_model\"),\n",
    "    \"clipper/tf_cifar_container:latest\",\n",
    "    [\"cifar\", \"tf\"],\n",
    "    \"doubles\",\n",
    "    num_containers=1\n",
    ")\n",
    "print(\"Model deploy successful? {success}\".format(success=model_added))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Clipper Metrics\n",
    "\n",
    "Clipper also records various system performance metrics. You can inspect the current state of these metrics with the `inspect_instance()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper.inspect_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Congratulations! You've now successfully completed the tutorial. You started Clipper, created an application and queried it from a frontend client, and deployed two models trained in two different machine learning frameworks (Scikit-Learn and TensorFlow) to the running system.__\n",
    "\n",
    "*Head back to the notebook from part 1. When you're done watching the accuracy of your application, stop the cell (hit the little \"stop\" square in the notebook toolbar).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/warning.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "> This step will stop and remove all Docker containers running on the host as well as delete all Docker images. You can run `clipper.stop_all()` instead which will stop all containers but not remove the images. If you have other containers running on your host, you should stop the Clipper containers manually.\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "\n",
    "__When you're completely done with the tutorial and want to shut down your Clipper instance, you can run the `cleanup()` command to stop all the Docker containers.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the accuracy of your frontend application a final time, you should see accuracy around 88%. If the accuracy is below that, you can try sending more feedback to increase the weight on the TensorFlow model even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
